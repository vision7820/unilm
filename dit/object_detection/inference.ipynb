{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from ditod import add_vit_config\n",
    "\n",
    "import torch\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.engine import DefaultPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"publaynet_configs/maskrcnn/maskrcnn_dit_base.yaml\"\n",
    "opts = ['MODEL.WEIGHTS', 'https://layoutlm.blob.core.windows.net/dit/dit-fts/publaynet_dit-b_mrcnn.pth']\n",
    "image = \"8.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(image):\n",
    "    # Step 1: instantiate config\n",
    "    cfg = get_cfg()\n",
    "    add_vit_config(cfg)\n",
    "    cfg.merge_from_file(config)\n",
    "\n",
    "    # Step 2: add model weights URL to config\n",
    "    cfg.merge_from_list(opts)\n",
    "\n",
    "    # Step 3: set device\n",
    "    device = \"cpu\"\n",
    "    cfg.MODEL.DEVICE = device\n",
    "\n",
    "    # Step 4: define model\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    # Step 5: run inference\n",
    "    img = cv2.imread(image)\n",
    "\n",
    "    md = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "    if cfg.DATASETS.TEST[0]=='icdar2019_test':\n",
    "        md.set(thing_classes=[\"table\"])\n",
    "    else:\n",
    "        md.set(thing_classes=[\"text\",\"title\",\"list\",\"table\",\"figure\"])\n",
    "        \n",
    "    output = predictor(img)[\"instances\"]\n",
    "        \n",
    "    v = Visualizer(img[:, :, ::-1],\n",
    "                md,\n",
    "                scale=1.0,\n",
    "                instance_mode=ColorMode.SEGMENTATION)\n",
    "    result = v.draw_instance_predictions(output.to(\"cpu\"))\n",
    "    result_image = result.get_image()[:, :, ::-1]\n",
    "    \n",
    "    return img, result_image, output.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hansh\\miniconda3\\envs\\unilm\\lib\\site-packages\\torch\\nn\\functional.py:3613: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "C:\\Users\\hansh\\miniconda3\\envs\\unilm\\lib\\site-packages\\torch\\utils\\checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "C:\\Users\\hansh\\miniconda3\\envs\\unilm\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "img, result_img, output = predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"out.jpg\", result_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=5, image_height=3508, image_width=2481, fields=[pred_boxes: Boxes(tensor([[ 376.8382,  771.7431, 2138.3201, 1209.8268],\n",
      "        [ 293.9623,  625.3801, 2155.1919,  742.9720],\n",
      "        [ 513.3420,  484.6617, 1727.9158,  560.6929],\n",
      "        [ 281.0919,  703.2103, 1778.7736,  748.9332],\n",
      "        [ 332.7250,  640.6341, 2148.9641,  686.6320]])), scores: tensor([0.9956, 0.9801, 0.9784, 0.3717, 0.2164]), pred_classes: tensor([2, 0, 1, 0, 0]), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]])])\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_index(instance):\n",
    "    bbox = instance.pred_boxes.tensor\n",
    "    sorting = sorted(range(bbox.size()[0]), key=lambda k: bbox[k][1].numpy())\n",
    "    return sorting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_tensor(tensor, sort_mask):\n",
    "    return tensor[sort_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort(instance):\n",
    "    sort_mask = sort_index(instance)\n",
    "    # sort pred_boxes\n",
    "    instance.pred_boxes.tensor = sort_tensor(instance.pred_boxes.tensor, sort_mask)\n",
    "    # sort score\n",
    "    instance.scores = sort_tensor(instance.scores, sort_mask)\n",
    "    # sort pred_classes\n",
    "    instance.pred_classes = sort_tensor(instance.pred_classes, sort_mask)\n",
    "    # sort pred_masks\n",
    "    instance.pred_masks = sort_tensor(instance.pred_masks, sort_mask)\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remove_mask(instance, conf):\n",
    "    scores = instance.scores\n",
    "    out_mask = []\n",
    "    for idx,score in enumerate(scores):\n",
    "        if score >= conf:\n",
    "            out_mask.append(idx)\n",
    "    return out_mask\n",
    "\n",
    "def filter_tensor(tensor, mask):\n",
    "    return tensor[mask]\n",
    "\n",
    "def remove_box_lower_than(instance, conf):\n",
    "    mask = get_remove_mask(instance, conf)\n",
    "    # sort pred_boxes\n",
    "    instance.pred_boxes.tensor = filter_tensor(instance.pred_boxes.tensor, mask)\n",
    "    # sort score\n",
    "    instance.scores = filter_tensor(instance.scores, mask)\n",
    "    # sort pred_classes\n",
    "    instance.pred_classes = filter_tensor(instance.pred_classes, mask)\n",
    "    # sort pred_masks\n",
    "    instance.pred_masks = filter_tensor(instance.pred_masks, mask)\n",
    "\n",
    "    return instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(488, 1812, 3)\n",
      "(167, 1912, 3)\n",
      "(126, 1264, 3)\n"
     ]
    }
   ],
   "source": [
    "def add_padding(input_img, padding, color):\n",
    "    old_image_height, old_image_width, channels = input_img.shape\n",
    "\n",
    "    # create new image of desired size and color (blue) for padding\n",
    "    new_image_width = old_image_width + 2 * padding\n",
    "    new_image_height = old_image_height + 2 * padding\n",
    "    result = np.full((new_image_height,new_image_width, channels), color, dtype=np.uint8)\n",
    "    print(result.shape)\n",
    "\n",
    "    # compute center offset\n",
    "    x_center = (new_image_width - old_image_width) // 2\n",
    "    y_center = (new_image_height - old_image_height) // 2\n",
    "\n",
    "    # copy img image into center of result image\n",
    "    result[y_center:y_center+old_image_height,\n",
    "           x_center:x_center+old_image_width] = input_img\n",
    "    return result\n",
    "\n",
    "def crop_and_save_image(input_img, bbox, filename, padding=25, color=(255,255,255)):\n",
    "    height, width, channels = input_img.shape\n",
    "    x1 = int(bbox[0])\n",
    "    x2 = int(bbox[2])\n",
    "    y1 = int(bbox[1])\n",
    "    y2 = int(bbox[3])\n",
    "    cropped_image = input_img[y1:y2, x1:x2]\n",
    "    padded_image = add_padding(cropped_image, padding, color)\n",
    "    cv2.imwrite(filename, padded_image)\n",
    "\n",
    "out_folder = \"out\"\n",
    "img = cv2.imread(image)\n",
    "\n",
    "remove_box_lower_than(output, 0.4)\n",
    "for idx, box in enumerate(output.pred_boxes.tensor):\n",
    "    crop_and_save_image(img, box, os.path.join(out_folder, str(idx) + \".jpg\"), 25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}