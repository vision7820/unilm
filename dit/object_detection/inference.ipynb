{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hansh\\miniconda3\\envs\\unilm\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "from ditod import add_vit_config\n",
    "\n",
    "import torch\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.engine import DefaultPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"icdar19_configs/maskrcnn/maskrcnn_dit_base.yaml\"\n",
    "opts = []\n",
    "image = \"img.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(image):\n",
    "    # Step 1: instantiate config\n",
    "    cfg = get_cfg()\n",
    "    add_vit_config(cfg)\n",
    "    cfg.merge_from_file(config)\n",
    "\n",
    "    # Step 2: add model weights URL to config\n",
    "    cfg.merge_from_list(opts)\n",
    "\n",
    "    # Step 3: set device\n",
    "    device = \"cpu\"\n",
    "    cfg.MODEL.DEVICE = device\n",
    "\n",
    "    # Step 4: define model\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    # Step 5: run inference\n",
    "    img = cv2.imread(image)\n",
    "\n",
    "    md = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "    if cfg.DATASETS.TEST[0]=='icdar2019_test':\n",
    "        md.set(thing_classes=[\"table\"])\n",
    "    else:\n",
    "        md.set(thing_classes=[\"text\",\"title\",\"list\",\"table\",\"figure\"])\n",
    "        \n",
    "    output = predictor(img)[\"instances\"]\n",
    "        \n",
    "    v = Visualizer(img[:, :, ::-1],\n",
    "                md,\n",
    "                scale=1.0,\n",
    "                instance_mode=ColorMode.SEGMENTATION)\n",
    "    result = v.draw_instance_predictions(output.to(\"cpu\"))\n",
    "    result_image = result.get_image()[:, :, ::-1]\n",
    "    \n",
    "    return img, result_image, output.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dit-base-224-p16-500k-62d53a.pth: 1.11GB [12:50, 1.44MB/s]                                                                                                                                                                             \n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.0.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.0.attn.qkv.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.0.attn.{q_bias, v_bias}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.0.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.0.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.0.{gamma_1, gamma_2}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.1.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.1.attn.qkv.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.1.attn.{q_bias, v_bias}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.1.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.1.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.1.{gamma_1, gamma_2}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.10.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.10.attn.qkv.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.10.attn.{q_bias, v_bias}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.10.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.10.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.10.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.10.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.10.{gamma_1, gamma_2}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.11.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.11.attn.qkv.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.11.attn.{q_bias, v_bias}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.11.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.11.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.11.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.11.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.11.{gamma_1, gamma_2}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.2.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.2.attn.qkv.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.2.attn.{q_bias, v_bias}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.2.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.2.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.2.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.2.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.2.{gamma_1, gamma_2}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.3.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.3.attn.qkv.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.3.attn.{q_bias, v_bias}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.3.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.3.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.3.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.3.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.3.{gamma_1, gamma_2}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.4.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.4.attn.qkv.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.4.attn.{q_bias, v_bias}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.4.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.4.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.4.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.4.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.4.{gamma_1, gamma_2}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.5.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.5.attn.qkv.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.5.attn.{q_bias, v_bias}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.5.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.5.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.5.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.5.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.5.{gamma_1, gamma_2}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.6.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.6.attn.qkv.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.6.attn.{q_bias, v_bias}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.6.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.6.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.6.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.6.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.6.{gamma_1, gamma_2}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.7.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.7.attn.qkv.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.7.attn.{q_bias, v_bias}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.7.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.7.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.7.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.7.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.7.{gamma_1, gamma_2}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.8.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.8.attn.qkv.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.8.attn.{q_bias, v_bias}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.8.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.8.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.8.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.8.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.8.{gamma_1, gamma_2}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.9.attn.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.9.attn.qkv.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.9.attn.{q_bias, v_bias}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.9.mlp.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.9.mlp.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.9.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.9.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.blocks.9.{gamma_1, gamma_2}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.fpn1.0.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.fpn1.1.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.fpn1.3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.fpn2.0.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.patch_embed.proj.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.backbone.{cls_token, pos_embed}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral4.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral5.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output4.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output5.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.conv.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.deconv.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn1.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn2.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn3.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn4.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mcls_token\u001b[0m\n",
      "  \u001b[35mmask_token\u001b[0m\n",
      "  \u001b[35mpos_embed\u001b[0m\n",
      "  \u001b[35mpatch_embed.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.0.{gamma_1, gamma_2}\u001b[0m\n",
      "  \u001b[35mblocks.0.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.0.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mblocks.0.attn.qkv.weight\u001b[0m\n",
      "  \u001b[35mblocks.0.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.0.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.0.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.0.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.1.{gamma_1, gamma_2}\u001b[0m\n",
      "  \u001b[35mblocks.1.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.1.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mblocks.1.attn.qkv.weight\u001b[0m\n",
      "  \u001b[35mblocks.1.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.1.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.1.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.1.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.2.{gamma_1, gamma_2}\u001b[0m\n",
      "  \u001b[35mblocks.2.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.2.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mblocks.2.attn.qkv.weight\u001b[0m\n",
      "  \u001b[35mblocks.2.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.2.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.2.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.2.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.3.{gamma_1, gamma_2}\u001b[0m\n",
      "  \u001b[35mblocks.3.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.3.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mblocks.3.attn.qkv.weight\u001b[0m\n",
      "  \u001b[35mblocks.3.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.3.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.3.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.3.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.4.{gamma_1, gamma_2}\u001b[0m\n",
      "  \u001b[35mblocks.4.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.4.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mblocks.4.attn.qkv.weight\u001b[0m\n",
      "  \u001b[35mblocks.4.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.4.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.4.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.4.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.5.{gamma_1, gamma_2}\u001b[0m\n",
      "  \u001b[35mblocks.5.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.5.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mblocks.5.attn.qkv.weight\u001b[0m\n",
      "  \u001b[35mblocks.5.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.5.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.5.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.5.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.6.{gamma_1, gamma_2}\u001b[0m\n",
      "  \u001b[35mblocks.6.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.6.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mblocks.6.attn.qkv.weight\u001b[0m\n",
      "  \u001b[35mblocks.6.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.6.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.6.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.6.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.7.{gamma_1, gamma_2}\u001b[0m\n",
      "  \u001b[35mblocks.7.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.7.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mblocks.7.attn.qkv.weight\u001b[0m\n",
      "  \u001b[35mblocks.7.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.7.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.7.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.7.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.8.{gamma_1, gamma_2}\u001b[0m\n",
      "  \u001b[35mblocks.8.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.8.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mblocks.8.attn.qkv.weight\u001b[0m\n",
      "  \u001b[35mblocks.8.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.8.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.8.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.8.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.9.{gamma_1, gamma_2}\u001b[0m\n",
      "  \u001b[35mblocks.9.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.9.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mblocks.9.attn.qkv.weight\u001b[0m\n",
      "  \u001b[35mblocks.9.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.9.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.9.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.9.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.10.{gamma_1, gamma_2}\u001b[0m\n",
      "  \u001b[35mblocks.10.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.10.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mblocks.10.attn.qkv.weight\u001b[0m\n",
      "  \u001b[35mblocks.10.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.10.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.10.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.10.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.11.{gamma_1, gamma_2}\u001b[0m\n",
      "  \u001b[35mblocks.11.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.11.attn.{q_bias, v_bias}\u001b[0m\n",
      "  \u001b[35mblocks.11.attn.qkv.weight\u001b[0m\n",
      "  \u001b[35mblocks.11.attn.proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.11.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.11.mlp.fc1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mblocks.11.mlp.fc2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mnorm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mlm_head.{bias, weight}\u001b[0m\n",
      "C:\\Users\\hansh\\miniconda3\\envs\\unilm\\lib\\site-packages\\torch\\nn\\functional.py:3613: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "C:\\Users\\hansh\\miniconda3\\envs\\unilm\\lib\\site-packages\\torch\\utils\\checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "C:\\Users\\hansh\\miniconda3\\envs\\unilm\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "img, result_img, output = predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances(num_instances=100, image_height=793, image_width=1104, fields=[pred_boxes: Boxes(tensor([[7.4608e+02, 7.3757e+02, 8.0659e+02, 7.9300e+02],\n",
      "        [7.2532e+02, 2.9953e+02, 7.5786e+02, 3.3130e+02],\n",
      "        [4.2741e+02, 1.3261e+02, 4.8939e+02, 1.9714e+02],\n",
      "        [3.4244e+02, 1.2849e+02, 4.0282e+02, 1.9047e+02],\n",
      "        [9.0393e+02, 3.8139e+02, 9.6609e+02, 4.4461e+02],\n",
      "        [6.4300e+02, 7.4757e+02, 6.7536e+02, 7.7855e+02],\n",
      "        [4.3586e+02, 7.3809e+02, 4.9816e+02, 7.9300e+02],\n",
      "        [3.4857e+02, 2.0689e+02, 4.1098e+02, 2.7029e+02],\n",
      "        [9.7498e+02, 3.8929e+02, 1.0364e+03, 4.5112e+02],\n",
      "        [6.3454e+02, 2.9999e+02, 6.6781e+02, 3.3129e+02],\n",
      "        [5.2234e+02, 6.6276e+02, 5.6376e+02, 6.8455e+02],\n",
      "        [3.5133e+02, 2.1827e+02, 3.9160e+02, 2.4053e+02],\n",
      "        [7.4872e+02, 7.0543e+02, 7.8796e+02, 7.2760e+02],\n",
      "        [7.2091e+02, 3.8996e+02, 7.8299e+02, 4.5154e+02],\n",
      "        [6.8965e+02, 3.8959e+02, 7.5160e+02, 4.5171e+02],\n",
      "        [4.7707e+02, 7.4711e+02, 5.3883e+02, 7.9300e+02],\n",
      "        [3.4702e+02, 2.2594e+02, 3.8827e+02, 2.4795e+02],\n",
      "        [2.8713e+02, 3.5362e-01, 3.4701e+02, 6.4174e+01],\n",
      "        [5.3140e+02, 6.9781e+02, 5.9347e+02, 7.6075e+02],\n",
      "        [7.9189e+02, 6.8597e+02, 8.3264e+02, 7.0787e+02],\n",
      "        [6.9759e+02, 3.0052e+02, 7.3017e+02, 3.3180e+02],\n",
      "        [7.3571e+02, 7.0621e+02, 7.9873e+02, 7.6981e+02],\n",
      "        [2.9803e+02, 0.0000e+00, 3.3809e+02, 1.3994e+01],\n",
      "        [9.3613e+02, 3.8189e+02, 9.9743e+02, 4.4355e+02],\n",
      "        [3.0942e+02, 1.6749e+02, 3.7085e+02, 2.2907e+02],\n",
      "        [7.1389e+02, 2.8616e+02, 7.7639e+02, 3.4929e+02],\n",
      "        [3.4781e+02, 7.1348e+02, 3.8863e+02, 7.3561e+02],\n",
      "        [4.8729e+02, 2.6122e+02, 5.2793e+02, 2.8360e+02],\n",
      "        [6.4602e+02, 6.8617e+02, 6.8635e+02, 7.0831e+02],\n",
      "        [3.4153e+02, 6.9945e+02, 4.0238e+02, 7.6056e+02],\n",
      "        [6.8963e+02, 7.3835e+02, 7.5198e+02, 7.9298e+02],\n",
      "        [4.2594e+02, 7.6789e+01, 4.9009e+02, 1.4319e+02],\n",
      "        [7.7612e+02, 7.2922e+02, 8.3799e+02, 7.9300e+02],\n",
      "        [7.1022e+02, 2.9948e+02, 7.4238e+02, 3.3092e+02],\n",
      "        [6.0238e+02, 3.8188e+02, 6.6463e+02, 4.4458e+02],\n",
      "        [7.6434e+02, 7.0549e+02, 8.0358e+02, 7.2740e+02],\n",
      "        [1.0155e+03, 6.5133e+02, 1.0757e+03, 7.1444e+02],\n",
      "        [3.8406e+02, 3.0938e+02, 4.2309e+02, 3.3127e+02],\n",
      "        [4.6039e+02, 6.5139e+02, 5.2210e+02, 7.1391e+02],\n",
      "        [4.1243e+02, 6.5024e+02, 4.7449e+02, 7.1421e+02],\n",
      "        [4.2760e+02, 3.8196e+02, 4.8985e+02, 4.4528e+02],\n",
      "        [3.8111e+02, 3.6449e+02, 4.4260e+02, 4.2810e+02],\n",
      "        [3.4899e+02, 2.5502e+02, 4.0978e+02, 3.1732e+02],\n",
      "        [8.7159e+02, 6.6710e+02, 9.3354e+02, 7.3049e+02],\n",
      "        [4.4497e+02, 2.5506e+02, 5.0545e+02, 3.1769e+02],\n",
      "        [5.2222e+02, 5.4694e+02, 5.6319e+02, 5.6903e+02],\n",
      "        [3.7948e+02, 7.1341e+02, 4.1956e+02, 7.3530e+02],\n",
      "        [7.4570e+02, 3.0009e+02, 7.7829e+02, 3.3156e+02],\n",
      "        [4.6649e+02, 7.1850e+01, 5.2921e+02, 1.3369e+02],\n",
      "        [4.5365e+02, 1.2552e+02, 5.1430e+02, 1.8902e+02],\n",
      "        [4.9939e+02, 2.9430e+02, 5.6229e+02, 3.5644e+02],\n",
      "        [9.6744e+02, 6.4978e+02, 1.0294e+03, 7.1382e+02],\n",
      "        [6.0405e+02, 7.4733e+02, 6.6567e+02, 7.9300e+02],\n",
      "        [5.3077e+02, 2.9431e+02, 5.9323e+02, 3.5620e+02],\n",
      "        [6.5448e+02, 7.0555e+02, 7.2034e+02, 7.6873e+02],\n",
      "        [8.0081e+02, 3.8991e+02, 8.6283e+02, 4.5142e+02],\n",
      "        [8.5669e+02, 2.5402e+02, 9.1754e+02, 3.1718e+02],\n",
      "        [8.0875e+02, 6.5190e+02, 8.7031e+02, 7.1411e+02],\n",
      "        [4.6271e+02, 1.0352e+02, 5.0342e+02, 1.2598e+02],\n",
      "        [6.1730e+02, 6.5281e+02, 6.7921e+02, 7.1483e+02],\n",
      "        [6.8959e+02, 2.9426e+02, 7.5189e+02, 3.5623e+02],\n",
      "        [6.6133e+02, 7.4284e+02, 7.0401e+02, 7.6427e+02],\n",
      "        [4.8808e+02, 1.7034e+02, 5.2869e+02, 1.9158e+02],\n",
      "        [9.5189e+02, 3.2478e+01, 1.0133e+03, 9.5472e+01],\n",
      "        [5.7117e+02, 3.0015e+02, 6.0442e+02, 3.3126e+02],\n",
      "        [7.6826e+02, 7.0560e+02, 8.2993e+02, 7.6771e+02],\n",
      "        [5.8378e+02, 7.3079e+02, 6.4671e+02, 7.9286e+02],\n",
      "        [4.0379e+02, 3.9028e+02, 4.6610e+02, 4.5195e+02],\n",
      "        [5.7172e+02, 2.8620e+02, 6.3381e+02, 3.4937e+02],\n",
      "        [5.2850e+02, 6.5230e+02, 5.9116e+02, 7.1500e+02],\n",
      "        [3.8827e+02, 2.9398e+02, 4.5148e+02, 3.5652e+02],\n",
      "        [4.6055e+02, 0.0000e+00, 5.2151e+02, 4.7987e+01],\n",
      "        [6.6589e+02, 6.7069e+02, 7.0681e+02, 6.9181e+02],\n",
      "        [9.2037e+02, 6.6710e+02, 9.8203e+02, 7.2998e+02],\n",
      "        [5.2194e+02, 6.5474e+02, 5.6348e+02, 6.7690e+02],\n",
      "        [3.8714e+02, 7.1278e+01, 4.5060e+02, 1.3407e+02],\n",
      "        [4.9094e+02, 6.9742e+02, 5.5285e+02, 7.6090e+02],\n",
      "        [5.0889e+02, 7.4714e+02, 5.7061e+02, 7.9300e+02],\n",
      "        [5.4697e+02, 3.8246e+02, 6.0982e+02, 4.4401e+02],\n",
      "        [4.5179e+02, 3.8968e+02, 5.1351e+02, 4.5161e+02],\n",
      "        [7.6484e+02, 6.8520e+02, 8.0510e+02, 7.0717e+02],\n",
      "        [5.5484e+02, 3.3859e+02, 6.1695e+02, 4.0249e+02],\n",
      "        [1.0471e+03, 6.1006e-01, 1.1040e+03, 6.3488e+01],\n",
      "        [4.7577e+02, 3.0030e+02, 5.0835e+02, 3.3149e+02],\n",
      "        [5.1537e+02, 3.9026e+02, 5.7740e+02, 4.5180e+02],\n",
      "        [7.3005e+02, 3.0288e+02, 7.9073e+02, 3.6530e+02],\n",
      "        [4.4441e+02, 6.9765e+02, 5.0475e+02, 7.6068e+02],\n",
      "        [6.5044e+02, 6.7057e+02, 6.9114e+02, 6.9187e+02],\n",
      "        [7.3255e+02, 7.1363e+02, 7.7312e+02, 7.3579e+02],\n",
      "        [5.2663e+02, 7.1304e+02, 5.6679e+02, 7.3559e+02],\n",
      "        [8.8840e+02, 2.5410e+02, 9.4959e+02, 3.1724e+02],\n",
      "        [7.5273e+02, 2.5431e+02, 8.1425e+02, 3.1528e+02],\n",
      "        [8.3180e+02, 2.6263e+02, 8.9383e+02, 3.2442e+02],\n",
      "        [1.0064e+03, 6.3529e+02, 1.0670e+03, 6.9765e+02],\n",
      "        [7.8385e+02, 7.3777e+02, 8.2414e+02, 7.5946e+02],\n",
      "        [5.7746e+02, 6.4315e+02, 6.4119e+02, 7.0737e+02],\n",
      "        [3.3148e+02, 7.1347e+02, 3.7188e+02, 7.3528e+02],\n",
      "        [9.9966e+02, 6.6715e+02, 1.0612e+03, 7.3014e+02],\n",
      "        [9.5120e+02, 6.6701e+02, 1.0134e+03, 7.2987e+02],\n",
      "        [6.6446e+02, 7.5026e+02, 7.0483e+02, 7.7148e+02]])), scores: tensor([0.5707, 0.5698, 0.5690, 0.5675, 0.5654, 0.5653, 0.5640, 0.5624, 0.5623,\n",
      "        0.5618, 0.5601, 0.5598, 0.5597, 0.5594, 0.5592, 0.5592, 0.5588, 0.5585,\n",
      "        0.5578, 0.5577, 0.5577, 0.5572, 0.5571, 0.5569, 0.5565, 0.5563, 0.5562,\n",
      "        0.5561, 0.5560, 0.5559, 0.5559, 0.5557, 0.5557, 0.5556, 0.5555, 0.5554,\n",
      "        0.5554, 0.5554, 0.5553, 0.5548, 0.5542, 0.5540, 0.5539, 0.5537, 0.5533,\n",
      "        0.5533, 0.5533, 0.5532, 0.5531, 0.5531, 0.5531, 0.5529, 0.5526, 0.5522,\n",
      "        0.5522, 0.5522, 0.5521, 0.5516, 0.5515, 0.5512, 0.5509, 0.5508, 0.5507,\n",
      "        0.5507, 0.5507, 0.5505, 0.5505, 0.5505, 0.5504, 0.5503, 0.5499, 0.5499,\n",
      "        0.5497, 0.5497, 0.5494, 0.5494, 0.5494, 0.5491, 0.5490, 0.5488, 0.5488,\n",
      "        0.5488, 0.5488, 0.5485, 0.5484, 0.5482, 0.5482, 0.5480, 0.5479, 0.5479,\n",
      "        0.5479, 0.5476, 0.5475, 0.5475, 0.5474, 0.5474, 0.5473, 0.5473, 0.5472,\n",
      "        0.5470]), pred_classes: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]])])\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
